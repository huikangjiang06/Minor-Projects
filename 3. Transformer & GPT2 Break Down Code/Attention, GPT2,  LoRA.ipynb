{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cf166-69df-41b9-b3f9-37f314fa4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn import datasets, linear_model, metrics, model_selection, preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0f180-1f1a-4005-802e-254c5508736d",
   "metadata": {},
   "source": [
    "### Implementing Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138bef64-0efe-4c67-b4c1-2cd67e3965cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))        ### W: [d_in, d_out]\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):                                           ### x : [batch_size, seq_length, d_in]\n",
    "        keys = x @ self.W_key                                       ### keys: [batch_size, seq_length, d_out], @: 矩阵乘法\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = torch.bmm(queries, keys.transpose(1, 2))      ### 计算注意力分数: [batch_size, seq_length, seq_length]\n",
    "        attn_weights = torch.softmax(attn_scores \n",
    "                                     / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values                         ### 输出： [batch_size, seq_length, output_dimension]\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1badf962-9492-442e-a2a4-83c6d572f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)        ### 没有偏置的线性层，[d_in, d_out]的矩阵，跟Parameter效果一样\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)                          ### 设置Dropout概率\n",
    "        self.register_buffer(                                       ### 设置一个上三角矩阵\n",
    "           'mask',\n",
    "           torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        b, num_tokens, d_in = q.shape\n",
    "        keys = self.W_key(k)\n",
    "        queries = self.W_query(q)\n",
    "        values = self.W_value(v)\n",
    "        attn_scores = queries @ keys.transpose(1, 2) \n",
    "        attn_scores.masked_fill_(                                   ### 取上三角矩阵的左上部分，然后把1换成-inf\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)   \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1) ### -inf经过softmax得0\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = torch.matmul(attn_weights,values)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913fe3b-1bca-43f8-9122-ce426b946e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(                                 ### 重复多头\n",
    "            CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        )\n",
    "    def forward(self, q, k, v):\n",
    "        return torch.cat([head(q, k, v) for head in self.heads], dim=-1)  ### 多头注意力的输出是拼接，不是相加，保留各头自己的特征，然后传给线性层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d1a8d-e311-4f38-852b-f9735f0c9e6e",
   "metadata": {},
   "source": [
    "### Implementing Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6be980-e5c0-4878-bcc4-573becc132c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_in, context_length, num_heads, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        d_out = d_in // num_heads\n",
    "        self.att = MultiheadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n",
    "        self.norm1 = nn.LayerNorm(d_in)\n",
    "        self.norm2 = nn.LayerNorm(d_in)\n",
    "        self.drop_resid = nn.Dropout(dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_in,d_in*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_in*4,d_in)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        value = x\n",
    "        x = self.att(x, x, x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + value\n",
    "        x = self.norm1(x)\n",
    "        value = x\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + value\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b70eb-bfdd-4f26-a51e-25d1d005bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_in, context_length, num_heads, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        d_out = int(d_in/num_heads)\n",
    "        self.att1 = MultiheadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n",
    "        self.att2 = MultiheadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n",
    "        self.norm1 = nn.LayerNorm(d_in)\n",
    "        self.norm2 = nn.LayerNorm(d_in)\n",
    "        self.norm3 = nn.LayerNorm(d_in)\n",
    "        self.drop_resid = nn.Dropout(dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_in, d_in*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_in*4, d_in)\n",
    "        )\n",
    "    def forward(self, x, enc_output):\n",
    "        value = x\n",
    "        x = self.att1(x, x, x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + value\n",
    "        x = self.norm1(x)\n",
    "        value = x\n",
    "        x = self.att2(x, enc_output, enc_output) \n",
    "        x = self.drop_resid(x)\n",
    "        x = x + value\n",
    "        x = self.norm2(x)\n",
    "        value = x\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + value\n",
    "        x = self.norm3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80edaff1-b214-4f98-b706-ff128d1245cd",
   "metadata": {},
   "source": [
    "### Implementing GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f017f-b9cd-40bb-bfca-37e4678a426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE_Expert(nn.Module):\n",
    "    def __init__(self,d_in,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.up=nn.Linear(d_in, 4*d_in, bias=False)\n",
    "        self.gate=nn.Linear(d_in, 4*d_in, bias=False)\n",
    "        self.down=nn.Linear(4*d_in, d_in, bias=False)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.act_fn=nn.GELU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.act_fn(self.gate(x)) * self.up(x)\n",
    "        x = self.down(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e3b68-1fd1-4552-af7c-860dbc800301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE_Router(nn.Module):\n",
    "    def __init__(self,d_in,num_experts,topk):\n",
    "        super().__init__()\n",
    "        self.topk = topk\n",
    "        self.ln = nn.Linear(d_in, num_experts)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.ln(x)\n",
    "        logits, indices = x.topk(self.topk, dim=-1)\n",
    "        logits = torch.full_like(x, float('-inf')).scatter_(-1, indices, logits)\n",
    "        weight = nn.functional.softmax(logits, dim=-1)\n",
    "        return weight, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5647e5-9a60-4db4-a803-5b8524be4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    def __init__(self,d_in,num_expers,topk,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.router=MoE_Router(d_in,num_expers,topk)\n",
    "        self.experts=nn.ModuleList(MoE_Expert(d_in,dropout) for _ in range(num_expers))\n",
    "\n",
    "    def forward(self,x):\n",
    "        weight, indices = self.router(x)\n",
    "        out = torch.zeros_like(x)\n",
    "        x_flat = x.reshape(-1, x.shape[-1])\n",
    "        weight_flat = weight.reshape(-1, weight.shape[-1])\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            mask = (indices == i).any(dim=-1)\n",
    "            mask_flat = mask.reshape(-1)\n",
    "            if mask.any():\n",
    "                outi = expert(x_flat[mask_flat])\n",
    "                score = weight_flat[mask_flat, i].unsqueeze(1)\n",
    "                outi = outi * score\n",
    "                out[mask] += outi.squeeze(1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58532c-29ce-4230-8b27-4ac8ea962de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Layer(nn.Module):\n",
    "    def __init__(self, d_in, context_length=1024, num_heads=12, use_moe=True, num_experts=4, topk=2, dropout=0.1, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        d_out = int(d_in/num_heads)\n",
    "        \n",
    "        self.drop_emb = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_in)\n",
    "        self.linear1 = nn.Linear(d_in,d_in*3)\n",
    "        self.att = MultiheadAttention(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n",
    "        self.drop_resid = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_in,d_in)\n",
    "        self.norm2 = nn.LayerNorm(d_in)\n",
    "        \n",
    "        if use_moe:\n",
    "            self.ff = MoE(d_in, num_experts, topk, dropout)\n",
    "        else:\n",
    "            self.ff = nn.Sequential(\n",
    "                nn.Linear(d_in, d_in*4),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(d_in*4, d_in),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = x\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.linear1(x)\n",
    "        q, k, v = x.split(self.d_in, dim=-1)\n",
    "        x = self.att(q, k, v)\n",
    "        x = self.linear2(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + value\n",
    "        \n",
    "        value = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + value\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf5222-55f6-4564-a93d-4b97c39bdb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1024):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pe = self.pe[:seq_len, :].unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "        x = x + pe\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f765201-e3bd-4676-99bc-037b31f62750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, num_layers=12, vocab_size=50257, hidden_size=768, context_length=1024, num_heads=12):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.layers = nn.Sequential(\n",
    "            *[GPT2Layer(hidden_size, context_length, num_heads, dropout=0, qkv_bias=False) \n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.ln=nn.Linear(hidden_size,vocab_size)\n",
    "        self.pe = PositionalEncoding(d_model=hidden_size,max_len=context_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.pe(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.ln(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e6e47-f976-4f38-bbcb-dc16009fa026",
   "metadata": {},
   "source": [
    "### Training GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c15fd-1526-4032-bf24-890ff0f3c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe768bc1-205a-4c94-a7c3-c11ef0337a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize\n",
    "import snowballstemmer\n",
    "import collections\n",
    "\n",
    "def load_data(path='/bohr/wikipedia-2l1k/v1/wikisent2.txt',max_len=50,vocab_size=10000):\n",
    "    texts=[]\n",
    "    line=[]\n",
    "    lens=[]\n",
    "    stemmer=snowballstemmer.stemmer('english')\n",
    "    counter=collections.Counter()\n",
    "    c=0               # 防止测试时等太久所以加上，之后可以删掉\n",
    "    with open(path,'rb') as f:\n",
    "        lines=tokenize.tokenize(f.readline)\n",
    "        for s in lines:\n",
    "            word=s.string.lower()\n",
    "            if word == 'utf-8':\n",
    "                continue\n",
    "            if word == '\\n':\n",
    "                texts.append(line)\n",
    "                lens.append(len(line))\n",
    "                line=[]\n",
    "                c+=1\n",
    "            else:\n",
    "                word=stemmer.stemWord(word)\n",
    "                line.append(word)\n",
    "                counter.update([word])\n",
    "            if c>=10000:\n",
    "                break\n",
    "    plt.hist(lens,bins=20)\n",
    "    str2idx={str:idx+2 for idx,(str,_) in enumerate(counter.most_common(vocab_size))}\n",
    "    str2idx['<UNK>'],str2idx['<PAD>']=0,1\n",
    "    idx2str={idx:str for str,idx in str2idx.items()}\n",
    "    texts=[[str2idx[str] if str in str2idx else str2idx['<UNK>'] for str in line] for line in texts]\n",
    "    texts=[line[:max_len] if len(line)>=max_len else line+(max_len-len(line))*[str2idx['<PAD>']] for line in texts]\n",
    "    texts=torch.tensor(texts,dtype=torch.long)\n",
    "    print(texts.shape)\n",
    "    return str2idx,idx2str,texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb5deb-cae5-4922-be58-93b630749a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_LAYERS=12\n",
    "VOCAB_SIZE=10000\n",
    "HIDDEN_SIZE=768\n",
    "CONTEXT_LENGTH=50\n",
    "NUM_HEADS=12\n",
    "BATCH_SIZE=16\n",
    "MAX_LEN=50\n",
    "PATH='/bohr/wikipedia-2l1k/v1/wikisent2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d3cdd-c091-47e9-a919-d1e1d722c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GPT2Model(NUM_LAYERS, VOCAB_SIZE, HIDDEN_SIZE, CONTEXT_LENGTH, NUM_HEADS).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d46a2e-4185-4b53-a704-f66e045f39bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "str2idx,idx2str,texts=load_data(PATH, MAX_LEN, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b493ba-7fc0-4ff3-9737-6b3c25a03bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=TensorDataset(texts)\n",
    "dataloader=DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5214c44-766b-49d9-b51e-be9b03ca287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ed684-0adf-46b5-b9e4-6e92edbf15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "optim_fn=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "for epoch in range(10):\n",
    "    losses=0\n",
    "    accs=0\n",
    "    batch_counter=0\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch + 1}')\n",
    "    for text in pbar:\n",
    "        text=text[0].cuda()\n",
    "        x,y=text[:,:-1],text[:,1:]\n",
    "        y_pred= model(x)\n",
    "        loss=loss_fn(y_pred.transpose(1,2),y)\n",
    "        optim_fn.zero_grad()\n",
    "        loss.backward()\n",
    "        optim_fn.step()\n",
    "        losses+=loss.item()\n",
    "        _,y_pred=torch.max(y_pred,dim=-1)\n",
    "        accs+=(y_pred==y).to(torch.float).mean()\n",
    "        batch_counter+=1\n",
    "    losses/=batch_counter\n",
    "    accs/=batch_counter\n",
    "    print(f'loss: {losses} acc: {accs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff1857-4853-4ede-b545-eb86592ff094",
   "metadata": {},
   "source": [
    "### Full Parameter Learning with Transformers and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb4c05-a161-481c-9da4-86e915cbbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"/personal/gpt2/\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"/personal/gpt2/\")\n",
    "\n",
    "'''\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\") ### 从huggingface直接下载数据\n",
    "直接下载时的数据结构：\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['text', 'label', 'input_ids', 'attention_mask', ...],\n",
    "        num_rows: 10000\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['text', 'label', 'input_ids', 'attention_mask', ...],\n",
    "        num_rows: 2000\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['text', 'label', 'input_ids', 'attention_mask', ...],\n",
    "        num_rows: 3000\n",
    "    })\n",
    "})\n",
    "'''\n",
    "\n",
    "### 从本地的.txt生成数据\n",
    "file_path = \"/personal/wikisent2.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "    dataset = Dataset.from_dict({\"text\": lines})\n",
    "\n",
    " ### 50个测试\n",
    "dataset = dataset.select(range(50))\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(\n",
    "        dataset[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "### 因模型而异，gpt2没有预设padding的编码\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "'''\n",
    "经过dataset.map之后的数据结构：\n",
    "Dataset({\n",
    "    features: ['input_ids', 'attention_mask'],\n",
    "    num_rows: 50\n",
    "})\n",
    "自动生成词表索引列和注意力掩码列，其中attention_mask 1 表示真实值，0 表示填充值。模型不会计算0位置的损失。可以手动更改attention_mask。\n",
    "'''\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "### 删去空数据\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda example: len(example['input_ids']) > 0)\n",
    "\n",
    "'''\n",
    "GPT-2期望的输入数据：\n",
    "{\n",
    "    \"input_ids\": torch.tensor([[50256, 1234, 5678, 9012, 50256]]),\n",
    "    \"attention_mask\": torch.tensor([[1, 1, 1, 1, 1]]),              \n",
    "    \"labels\": torch.tensor([[1234, 5678, 9012, 50256, -100]]) \n",
    "}\n",
    "'''\n",
    "### 加入labels列，和input一样，模型自动自回归\n",
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])\n",
    "\n",
    "### train test split\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = tokenized_dataset['train']\n",
    "test_dataset = tokenized_dataset['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "### training\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./model\")\n",
    "tokenizer.save_pretrained(\"./model\")\n",
    "\n",
    "### 预测\n",
    "prompt = \"Jack went to school yesterday and \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=50,\n",
    "    num_return_sequences=3,\n",
    "    num_beams=3,\n",
    "    do_sample=True,\n",
    "    temperature=3.0,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for i, sample in enumerate(output):\n",
    "    print(f\"{i+1}: {tokenizer.decode(sample, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53253839-80e9-465d-a34b-25c27226b725",
   "metadata": {},
   "source": [
    "### Supervised Fine Tuning on Q/A Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2344e-7f71-4458-8a29-e308436fd43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 假设现在本地有准备好的问答对数据，用们微调GPT-2来让他学习这些问答对\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_length=128):\n",
    "    inputs = examples[\"prompt\"]\n",
    "    targets = examples[\"response\"]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def train(model, tokenzier, dataset, output_dir=\"./sft_model\"):\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda dataset: preprocess_function(dataset, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        save_steps=500,\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"no\",\n",
    "        overwrite_output_dir=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b85176-3ce4-4537-a8d5-add381c2d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/jianggh/Desktop/IOAI/轻量级十一学校信息问答语言模型/问答对.xlsx\"\n",
    "df = pd.read_excel(file_path, header=None, names=['prompt', 'response'])\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "train(model,tokenzier,dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c5c20-735d-4211-ac7f-7d7c82b116a4",
   "metadata": {},
   "source": [
    "### LoRA Fine Tuning with PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb50819-38db-44be-9066-d63ce5cf082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import peft, PEFT是huggingface提供的进行微调的库，包括lora，但是目前比赛并没有提供这个PEFT，所以我们用PyTorch来实现lora\n",
    "\n",
    "### 定义lora网络结构\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.rank = rank  ### LoRA的秩（rank），控制低秩矩阵的大小\n",
    "        self.scaling = alpha ### 用来控制lora层的scaling参数\n",
    "        self.A = nn.Linear(in_features, rank, bias=False)  ### 低秩矩阵A\n",
    "        self.B = nn.Linear(rank, out_features, bias=False)  ### 低秩矩阵B\n",
    "        \n",
    "        self.A.weight.data.normal_(mean=0.0, std=0.02) ### 矩阵参数初始化\n",
    "        self.B.weight.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.B(self.A(x)) * self.scaling\n",
    "\n",
    "### 在输入模型中加入lora层\n",
    "def apply_lora(model, \n",
    "               layer_names=[\"query\", \"key\", \"value\", \"dense\"], ### 选择需要lora的原始module\n",
    "               rank=8, \n",
    "               alpha=1/16):\n",
    "    for name, module in model.named_modules(): ### 遍历model中的所有module\n",
    "        if isinstance(module, nn.Linear) and any(key in name for key in layer_names): ### 选择需要lora的原始module，可以更改这个条件\n",
    "            \n",
    "            lora = LoRA(module.in_features, module.out_features, rank, alpha).to(model.device)\n",
    "            setattr(module, \"lora\", lora) ### 将lora实例设置为module的属性，并命名为\"lora\"，以便后续识别\n",
    "            original_forward = module.forward\n",
    "\n",
    "            for param in module.parameters(): ### 冻结原始module的权重\n",
    "                param.requires_grad = False\n",
    "\n",
    "            def forward_with_lora(x, layer1=original_forward, layer2=lora): ### 将原来的foward函数替代\n",
    "                return layer1(x) + layer2(x)\n",
    "\n",
    "            module.forward = forward_with_lora\n",
    "\n",
    "### 只保存lora层\n",
    "### 提交比赛的时候也可以直接torch.save(model.state_dict())\n",
    "def save_lora(model, path):\n",
    "    state_dict = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'lora'): ### 此时判断module是否是lora，只保存lora\n",
    "            lora_state = {f'{name}.lora.{k}': v for k, v in module.lora.state_dict().items()} ### 此时遍历，k是参数名，v是参数值，保存\"{name}.lora.{k}\"确定参数位置\n",
    "            state_dict.update(lora_state)\n",
    "    torch.save(state_dict, path)\n",
    "\n",
    "def load_lora(model, path):\n",
    "    state_dict = torch.load(path, map_location=model.device)\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'lora'):\n",
    "            lora_state = {k.replace(f'{name}.lora.', ''): v for k, v in state_dict.items() if f'{name}.lora.' in k}\n",
    "            module.lora.load_state_dict(lora_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a47e3-acac-4dc2-a69a-b616759e26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 假设我们要完成一个分类任务，我们可以这样添加一个线性层在模型后面\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, model, num_labels):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.classifier= nn.Linear(model.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids, attention_mask=attention_mask)\n",
    "        output = outputs.last_hidden_state[:, 0]\n",
    "        return self.classifier(output)\n",
    "\n",
    "\n",
    "class TweetClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"text\"]\n",
    "        label = self.data[idx][\"target\"]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train(model, train_loader, optimizer, device, epochs=3):\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            masks = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, masks)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b593b3-53e0-426b-9828-3523a9fe8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/personal/gpt2/\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=2) ### 使用ForSequenceClassification而不是ForCausalLM，这样子会自动添加线性层\n",
    "### 亦可 model = ClassificationModel(model,2)\n",
    "apply_lora(model)\n",
    "\n",
    "### 某一个序列分类问题\n",
    "dataset = datasets.load_dataset(\"mehdiiraqui/twitter_disaster\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"val\"]\n",
    "\n",
    "train_dataset = TweetClassificationDataset(train_data, tokenizer, max_length=128)\n",
    "val_dataset = TweetClassificationDataset(val_data, tokenizer, max_length=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "### 确保原始modules不参与梯度计算\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name and \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "lora_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name or \"classifier\" in name:\n",
    "        lora_params.append(param)\n",
    "        \n",
    "### 只需要给optimizer喂lora层\n",
    "optimizer = optim.AdamW(lora_params, lr=5e-5)\n",
    "\n",
    "train(model, train_loader, optimizer, device='cuda', epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
