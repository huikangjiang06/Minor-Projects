{"cells":[{"id":"9a55bb20-e801-464c-9129-4fe5f87d5455","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport random\n\n# 设置随机种子\ndef init_seed(seed):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n# 简化版数据集类\nclass SimpleSkeletonDataset(Dataset):\n    def __init__(self, x, y):\n        # 转换数据格式: (n, c, t, v, m) -> (n, c, t, v)\n        self.x = x\n        # 转换标签: (n, 3) -> (n,) 类别索引\n        self.y = np.argmax(y, axis=1)  # 从one-hot转类别索引\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        return torch.FloatTensor(self.x[idx]), torch.LongTensor([self.y[idx]]).squeeze()\n\n# 更简单的模型结构\nclass SimpleHDGCN(nn.Module):\n    def __init__(self, num_class=3):\n        super().__init__()\n        # 输入形状: (batch, 3, 64, 17)\n        self.net = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),  # -> (64, 32, 8)\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),  # -> (128, 16, 4)\n            nn.Flatten(),\n            nn.Linear(128*16*4, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_class)\n        )\n        \n    def forward(self, x):\n        x = x[:, :, :64, :, 0]  # 取前64帧和第一个人的数据\n        return self.net(x)\n\n# 训练函数\ndef simple_train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss, correct = 0, 0\n    \n    for data, label in tqdm(loader, desc='Training'):\n        data, label = data.to(device), label.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, label)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        correct += (output.argmax(1) == label).sum().item()\n    \n    return total_loss/len(loader), correct/len(loader.dataset)\n\n# 测试函数\ndef simple_test(model, loader, criterion, device):\n    model.eval()\n    total_loss, correct = 0, 0\n    \n    with torch.no_grad():\n        for data, label in tqdm(loader, desc='Testing'):\n            data, label = data.to(device), label.to(device)\n            \n            output = model(data)\n            loss = criterion(output, label)\n            \n            total_loss += loss.item()\n            correct += (output.argmax(1) == label).sum().item()\n    \n    return total_loss/len(loader), correct/len(loader.dataset)\n\n# 主流程\nif __name__ == '__main__':\n    # 初始化\n    init_seed(1)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # 加载数据\n    print(\"Loading data...\")\n    data = np.load('/bohr/finefs-9pcr/v3/FS_data_for_HDGCN_jump.npz')\n    train_set = SimpleSkeletonDataset(data['x_train'], data['y_train'])\n    test_set = SimpleSkeletonDataset(data['x_test'], data['y_test'])\n    \n    # 数据加载器\n    train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4)\n    test_loader = DataLoader(test_set, batch_size=32, num_workers=4)\n    \n    # 模型和优化器\n    print(\"Initializing model...\")\n    model = SimpleHDGCN(num_class=17).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # 打印模型结构\n    print(model)\n    \n    # 训练循环\n    print(\"Starting training...\")\n    for epoch in range(30):  # 简化为20个epoch\n        train_loss, train_acc = simple_train(model, train_loader, criterion, optimizer, device)\n        test_loss, test_acc = simple_test(model, test_loader, criterion, device)\n        \n        print(f'\\nEpoch {epoch+1}:')\n        print(f'Train Loss: {train_loss:.4f}, Acc: {train_acc:.2%}')\n        print(f'Test Loss: {test_loss:.4f}, Acc: {test_acc:.2%}')","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_1581","output_type":"stream","name":"stdout","text":"Using device: cuda\nLoading data...\nInitializing model...\nSimpleHDGCN(\n  (net): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=8192, out_features=256, bias=True)\n    (8): ReLU()\n    (9): Linear(in_features=256, out_features=17, bias=True)\n  )\n)\nStarting training...\nTraining: 100%|██████████| 97/97 [00:00<00:00, 273.16it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 75.56it/s]\n\nEpoch 1:\nTrain Loss: 2.3789, Acc: 16.24%\nTest Loss: 2.2116, Acc: 24.39%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 271.22it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 82.03it/s]\n\nEpoch 2:\nTrain Loss: 2.2422, Acc: 25.02%\nTest Loss: 1.9582, Acc: 28.66%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 280.16it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 76.54it/s]\n\nEpoch 3:\nTrain Loss: 2.0091, Acc: 35.23%\nTest Loss: 1.9044, Acc: 32.32%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 278.04it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 81.17it/s]\n\nEpoch 4:\nTrain Loss: 1.7504, Acc: 44.20%\nTest Loss: 1.6674, Acc: 40.85%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 276.89it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 77.14it/s]\n\nEpoch 5:\nTrain Loss: 1.5237, Acc: 50.53%\nTest Loss: 1.6230, Acc: 39.63%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 277.89it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 76.57it/s]\n\nEpoch 6:\nTrain Loss: 1.3120, Acc: 56.60%\nTest Loss: 1.6092, Acc: 44.51%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 270.21it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 79.39it/s]\n\nEpoch 7:\nTrain Loss: 1.1247, Acc: 62.25%\nTest Loss: 1.5107, Acc: 46.34%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 273.30it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 77.25it/s]\n\nEpoch 8:\nTrain Loss: 0.9589, Acc: 67.55%\nTest Loss: 1.5383, Acc: 46.34%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 273.23it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 82.06it/s]\n\nEpoch 9:\nTrain Loss: 0.7538, Acc: 74.65%\nTest Loss: 1.7662, Acc: 43.29%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 274.37it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 78.20it/s]\n\nEpoch 10:\nTrain Loss: 0.5804, Acc: 80.72%\nTest Loss: 1.9003, Acc: 45.73%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 272.65it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 76.31it/s]\n\nEpoch 11:\nTrain Loss: 0.4197, Acc: 85.70%\nTest Loss: 1.9038, Acc: 48.17%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 273.21it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 79.22it/s]\n\nEpoch 12:\nTrain Loss: 0.2509, Acc: 92.57%\nTest Loss: 2.1790, Acc: 45.73%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 276.44it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 77.69it/s]\n\nEpoch 13:\nTrain Loss: 0.1607, Acc: 95.77%\nTest Loss: 2.3976, Acc: 48.17%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 279.20it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 77.73it/s]\n\nEpoch 14:\nTrain Loss: 0.0889, Acc: 98.09%\nTest Loss: 2.5339, Acc: 50.61%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 282.12it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 81.85it/s]\n\nEpoch 15:\nTrain Loss: 0.0583, Acc: 98.84%\nTest Loss: 2.7146, Acc: 49.39%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 273.10it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 76.21it/s]\n\nEpoch 16:\nTrain Loss: 0.0382, Acc: 99.35%\nTest Loss: 3.1235, Acc: 45.12%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 280.51it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 74.23it/s]\n\nEpoch 17:\nTrain Loss: 0.0225, Acc: 99.68%\nTest Loss: 3.0784, Acc: 48.78%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 275.28it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 75.83it/s]\n\nEpoch 18:\nTrain Loss: 0.0117, Acc: 99.87%\nTest Loss: 3.2150, Acc: 49.39%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 281.55it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 77.00it/s]\n\nEpoch 19:\nTrain Loss: 0.0091, Acc: 99.90%\nTest Loss: 3.2705, Acc: 47.56%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 271.22it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 80.55it/s]\n\nEpoch 20:\nTrain Loss: 0.0068, Acc: 99.87%\nTest Loss: 3.4048, Acc: 47.56%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 278.91it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 79.26it/s]\n\nEpoch 21:\nTrain Loss: 0.0071, Acc: 99.87%\nTest Loss: 3.4171, Acc: 48.17%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 268.19it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 80.95it/s]\n\nEpoch 22:\nTrain Loss: 0.0066, Acc: 99.87%\nTest Loss: 3.4723, Acc: 48.78%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 277.47it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 77.56it/s]\n\nEpoch 23:\nTrain Loss: 0.0055, Acc: 99.87%\nTest Loss: 3.6062, Acc: 48.17%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 278.32it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 74.56it/s]\n\nEpoch 24:\nTrain Loss: 0.0053, Acc: 99.87%\nTest Loss: 3.6945, Acc: 47.56%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 269.71it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 77.81it/s]\n\nEpoch 25:\nTrain Loss: 0.0047, Acc: 99.87%\nTest Loss: 3.7338, Acc: 48.17%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 271.55it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 77.20it/s]\n\nEpoch 26:\nTrain Loss: 0.0044, Acc: 99.87%\nTest Loss: 3.7355, Acc: 48.17%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 281.36it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 79.61it/s]\n\nEpoch 27:\nTrain Loss: 0.0039, Acc: 99.90%\nTest Loss: 3.7472, Acc: 48.78%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 279.18it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 77.28it/s]\n\nEpoch 28:\nTrain Loss: 0.0043, Acc: 99.87%\nTest Loss: 3.7711, Acc: 48.78%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 271.53it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 81.84it/s]\n\nEpoch 29:\nTrain Loss: 0.0041, Acc: 99.87%\nTest Loss: 3.9340, Acc: 47.56%\nTraining: 100%|██████████| 97/97 [00:00<00:00, 282.04it/s]\nTesting: 100%|██████████| 6/6 [00:00<00:00, 77.32it/s]\nEpoch 30:\nTrain Loss: 0.0037, Acc: 99.87%\nTest Loss: 3.8985, Acc: 47.56%\n\n","data":{"name":"stdout","text":"Using device: cuda\nLoading data...\nInitializing model...\nSimpleHDGCN(\n  (net): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=8192, out_features=256, bias=True)\n    (8): ReLU()\n    (9): Linear(in_features=256, out_features=17, bias=True)\n  )\n)\nStarting training...\n"},"meta":{},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_1581","msg_type":"stream","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:23:08.584572Z","version":"5.3"}},{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_1882","output_type":"execute_reply","data":{"status":"ok","execution_count":12,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:23:08.462889Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_1882","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:23:21.549365Z","version":"5.3"}}],"execution_count":12},{"id":"b888bd0d-83f1-46d0-ace0-5c8beebf7cac","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nimport random\n\n# 设置随机种子\ndef init_seed(seed):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n# 简化版数据集类\nclass SimpleSkeletonDataset(Dataset):\n    def __init__(self, x, y):\n        # 转换数据格式: (n, c, t, v, m) -> (n, c, t, v)\n        self.x = x\n        # 转换标签: (n, 3) -> (n,) 类别索引\n        self.y = np.argmax(y, axis=1)  # 从one-hot转类别索引\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        return torch.FloatTensor(self.x[idx]), torch.LongTensor([self.y[idx]]).squeeze()\n\n# 更简单的模型结构\nclass SimpleHDGCN(nn.Module):\n    def __init__(self, num_class=3):\n        super().__init__()\n        # 输入形状: (batch, 3, 64, 17)\n        self.net = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),  # -> (64, 32, 8)\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),  # -> (128, 16, 4)\n            nn.Flatten(),\n            nn.Linear(128*16*4, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_class)\n        )\n        \n    def forward(self, x):\n        x = x[:, :, :64, :, 0]  # 取前64帧和第一个人的数据\n        return self.net(x)\n\n# 训练函数\ndef simple_train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss, correct = 0, 0\n    \n    for data, label in tqdm(loader, desc='Training'):\n        data, label = data.to(device), label.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, label)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        correct += (output.argmax(1) == label).sum().item()\n    \n    return total_loss/len(loader), correct/len(loader.dataset)\n\n# 测试函数\ndef simple_test(model, loader, criterion, device):\n    model.eval()\n    total_loss, correct = 0, 0\n    \n    with torch.no_grad():\n        for data, label in tqdm(loader, desc='Testing'):\n            data, label = data.to(device), label.to(device)\n            \n            output = model(data)\n            loss = criterion(output, label)\n            \n            total_loss += loss.item()\n            correct += (output.argmax(1) == label).sum().item()\n    \n    return total_loss/len(loader), correct/len(loader.dataset)","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_200","output_type":"execute_reply","data":{"status":"ok","execution_count":2,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:17:06.091946Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_200","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.098285Z","version":"5.3"}}],"execution_count":2},{"id":"a6d520ed-75c0-4d63-acf5-5c781cca7c15","cell_type":"code","source":"def edge2mat(link, num_node):\n    A = np.zeros((num_node, num_node))\n    for i, j in link:\n        A[j, i] = 1\n    return A\n\ndef normalize_digraph(A):\n    Dl = np.sum(A, 0)\n    h, w = A.shape\n    Dn = np.zeros((w, w))\n    for i in range(w):\n        if Dl[i] > 0:\n            Dn[i, i] = Dl[i] ** (-1)\n    AD = np.dot(A, Dn)\n    return AD\n\ndef get_spatial_graph(num_node, hierarchy):\n    A = []\n    for i in range(len(hierarchy)):\n        A.append(normalize_digraph(edge2mat(hierarchy[i], num_node)))\n\n    A = np.stack(A)\n\n    return A\n\ndef get_spatial_graph_original(num_node, self_link, inward, outward):\n    I = edge2mat(self_link, num_node)\n    In = normalize_digraph(edge2mat(inward, num_node))\n    Out = normalize_digraph(edge2mat(outward, num_node))\n    A = np.stack((I, In, Out))\n    return A\n\ndef normalize_adjacency_matrix(A):\n    node_degrees = A.sum(-1)\n    degs_inv_sqrt = np.power(node_degrees, -0.5)\n    norm_degs_matrix = np.eye(len(node_degrees)) * degs_inv_sqrt\n    return (norm_degs_matrix @ A @ norm_degs_matrix).astype(np.float32)\n\ndef get_graph(num_node, edges):\n\n    I = edge2mat(edges[0], num_node)\n    Forward = normalize_digraph(edge2mat(edges[1], num_node))\n    Reverse = normalize_digraph(edge2mat(edges[2], num_node))\n    A = np.stack((I, Forward, Reverse))\n    return A # 3, 25, 25\n\ndef get_hierarchical_graph(num_node, edges):\n    A = []\n    for edge in edges:\n        A.append(get_graph(num_node, edge))\n    A = np.stack(A)\n    return A\n\ndef get_groups(dataset='NTU', CoM=1):\n    groups  =[]\n    \n    if dataset == 'NTU':\n        if CoM == 2:\n            groups.append([2])\n            groups.append([1, 21])\n            groups.append([13, 17, 3, 5, 9])\n            groups.append([14, 18, 4, 6, 10])\n            groups.append([15, 19, 7, 11])\n            groups.append([16, 20, 8, 12])\n            groups.append([22, 23, 24, 25])\n\n        ## Center of mass : 21\n        elif CoM == 21:\n            groups.append([21])\n            groups.append([2, 3, 5, 9])\n            groups.append([4, 6, 10, 1])\n            groups.append([7, 11, 13, 17])\n            groups.append([8, 12, 14, 18])\n            groups.append([22, 23, 24, 25, 15, 19])\n            groups.append([16, 20])\n\n        ## Center of Mass : 1\n        elif CoM == 1:\n            groups.append([1])\n            groups.append([2, 13, 17])\n            groups.append([14])\n            groups.append([3, 5, 9, 15])\n            groups.append([4, 6, 10, 16])\n            groups.append([7, 11])\n            groups.append([8, 12])\n\n        elif CoM == 8:\n            groups.append([8])\n            groups.append([1, 9])\n            groups.append([2, 5, 12, 15])\n            groups.append([3, 6, 13, 16])\n            groups.append([4, 7, 14, 17])\n            groups.append([10, 11])\n\n\n        else:\n            raise ValueError()\n        \n    return groups\n\ndef get_edgeset(dataset='NTU', CoM=1):\n    groups = get_groups(dataset=dataset, CoM=CoM)\n    \n    for i, group in enumerate(groups):\n        group = [i - 1 for i in group]\n        groups[i] = group\n\n    identity = []\n    forward_hierarchy = []\n    reverse_hierarchy = []\n\n    for i in range(len(groups) - 1):\n        self_link = groups[i] + groups[i + 1]\n        self_link = [(i, i) for i in self_link]\n        identity.append(self_link)\n        forward_g = []\n        for j in groups[i]:\n            for k in groups[i + 1]:\n                forward_g.append((j, k))\n        forward_hierarchy.append(forward_g)\n        \n        reverse_g = []\n        for j in groups[-1 - i]:\n            for k in groups[-2 - i]:\n                reverse_g.append((j, k))\n        reverse_hierarchy.append(reverse_g)\n\n    edges = []\n    for i in range(len(groups) - 1):\n        edges.append([identity[i], forward_hierarchy[i], reverse_hierarchy[-1 - i]])\n\n    return edges","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_204","output_type":"execute_reply","data":{"status":"ok","execution_count":3,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:17:06.099288Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_204","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.108086Z","version":"5.3"}}],"execution_count":3},{"id":"491a8873-caa4-4d55-b3bf-dbbc573043c2","cell_type":"code","source":"num_node = 17\nclass Graph:\n    def __init__(self, CoM=1, labeling_mode='spatial'):\n        self.num_node = num_node\n        self.CoM = CoM\n        self.A = self.get_adjacency_matrix(labeling_mode)\n        \n\n    def get_adjacency_matrix(self, labeling_mode=None):\n        if labeling_mode is None:\n            return self.A\n        if labeling_mode == 'spatial':\n            A = get_hierarchical_graph(num_node, get_edgeset(dataset='NTU', CoM=self.CoM))\n        else:\n            raise ValueError()\n        return A, self.CoM","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_208","output_type":"execute_reply","data":{"status":"ok","execution_count":4,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:17:06.109408Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_208","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.112251Z","version":"5.3"}}],"execution_count":4},{"id":"7b6a271d-0351-4ae4-a2ca-078863075fdd","cell_type":"code","source":"from torch.autograd import Variable\n\ndef import_class(name):\n    components = name.split('.')\n    mod = __import__(components[0])\n    for comp in components[1:]:\n        mod = getattr(mod, comp)\n    return mod\n\n\ndef conv_branch_init(conv, branches):\n    weight = conv.weight\n    n = weight.size(0)\n    k1 = weight.size(1)\n    k2 = weight.size(2)\n    nn.init.normal_(weight, 0, math.sqrt(2. / (n * k1 * k2 * branches)))\n    if conv.bias is not None:\n        nn.init.constant_(conv.bias, 0)\n\n\ndef conv_init(conv):\n    if conv.weight is not None:\n        nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n    if conv.bias is not None:\n        nn.init.constant_(conv.bias, 0)\n\n\ndef bn_init(bn, scale):\n    nn.init.constant_(bn.weight, scale)\n    nn.init.constant_(bn.bias, 0)\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        if hasattr(m, 'weight'):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n        if hasattr(m, 'bias') and m.bias is not None and isinstance(m.bias, torch.Tensor):\n            nn.init.constant_(m.bias, 0)\n    elif classname.find('BatchNorm') != -1:\n        if hasattr(m, 'weight') and m.weight is not None:\n            m.weight.data.normal_(1.0, 0.02)\n        if hasattr(m, 'bias') and m.bias is not None:\n            m.bias.data.fill_(0)\n            \n            \nclass TemporalConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n        super(TemporalConv, self).__init__()\n        pad = (kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=(kernel_size, 1),\n            padding=(pad, 0),\n            stride=(stride, 1),\n            dilation=(dilation, 1),\n            bias=False)\n        self.bias = nn.Parameter(torch.zeros(1, out_channels, 1, 1), requires_grad=True)\n\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x) + self.bias\n        x = self.bn(x)\n        return x\n\n\nclass MultiScale_TemporalConv(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=5,\n                 stride=1,\n                 dilations=[1,2],\n                 residual=True,\n                 residual_kernel_size=1):\n\n        super().__init__()\n        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n\n        # Multiple branches of temporal convolution\n        self.num_branches = len(dilations) + 2\n        branch_channels = out_channels // self.num_branches\n        if type(kernel_size) == list:\n            assert len(kernel_size) == len(dilations)\n        else:\n            kernel_size = [kernel_size] * len(dilations)\n        # Temporal Convolution branches\n        self.branches = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(\n                    in_channels,\n                    branch_channels,\n                    kernel_size=1,\n                    padding=0),\n                nn.BatchNorm2d(branch_channels),\n                nn.ReLU(inplace=True),\n                TemporalConv(\n                    branch_channels,\n                    branch_channels,\n                    kernel_size=ks,\n                    stride=stride,\n                    dilation=dilation),\n            )\n            for ks, dilation in zip(kernel_size, dilations)\n        ])\n\n        # Additional Max & 1x1 branch\n        self.branches.append(nn.Sequential(\n            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),\n            nn.BatchNorm2d(branch_channels),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=(3, 1), stride=(stride, 1), padding=(1, 0)),\n            nn.BatchNorm2d(branch_channels)\n        ))\n\n        self.branches.append(nn.Sequential(\n            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0, stride=(stride, 1)),\n            nn.BatchNorm2d(branch_channels)\n        ))\n\n        # Residual connection\n        if not residual:\n            self.residual = lambda x: 0\n        elif (in_channels == out_channels) and (stride == 1):\n            self.residual = lambda x: x\n        else:\n            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n\n        # initialize\n        self.apply(weights_init)\n\n    def forward(self, x):\n        branch_outs = []\n        for tempconv in self.branches:\n            out = tempconv(x)\n            branch_outs.append(out)\n\n        out = torch.cat(branch_outs, dim=1)\n        out += self.residual(x)\n        return out\n\n\nclass residual_conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n        super(residual_conv, self).__init__()\n        pad = int((kernel_size - 1) / 2)\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n                              stride=(stride, 1))\n\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        conv_init(self.conv)\n        bn_init(self.bn, 1)\n\n    def forward(self, x):\n        x = self.bn(self.conv(x))\n        return x\n\nclass EdgeConv(nn.Module):\n    def __init__(self, in_channels, out_channels, k):\n        super(EdgeConv, self).__init__()\n        \n        self.k = k\n        \n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels*2, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(inplace=True, negative_slope=0.2)\n        )\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                conv_init(m)\n            elif isinstance(m, nn.BatchNorm2d):\n                bn_init(m, 1)\n    \n    def forward(self, x, dim=4): # N, C, T, V\n        \n        if dim == 3:\n            N, C, L = x.size()\n            pass\n        else:\n            N, C, T, V = x.size()\n            x = x.mean(dim=-2, keepdim=False) # N, C, V\n        \n        x = self.get_graph_feature(x, self.k)\n        x = self.conv(x)\n        x = x.max(dim=-1, keepdim=False)[0]\n        \n        if dim == 3:\n            pass\n        else:\n            x = x.unsqueeze(2).expand(-1, -1, T, -1)\n        \n        return x\n        \n    def knn(self, x, k):\n        x = x.transpose(1, 2)  # N, V, C\n        inner = -2 * torch.matmul(x, x.transpose(2, 1)) # N, V, V\n        xx = torch.sum(x**2, dim=2, keepdim=True)\n        pairwise_distance = -xx - inner - xx.transpose(2, 1)\n        \n        idx = pairwise_distance.topk(k=k, dim=-1)[1] # N, V, k\n        return idx\n    \n    def get_graph_feature(self, x, k, idx=None):\n        N, C, V = x.size()\n        if idx is None:\n            idx = self.knn(x, k=k)\n        device = x.device\n        \n        idx_base = torch.arange(0, N, device=device).view(-1, 1, 1) * V\n        \n        idx = idx + idx_base\n        idx = idx.view(-1)\n        \n        x = x.transpose(1, 2).contiguous()  # N, V, C\n        feature = x.view(-1, C)[idx, :]\n        feature = feature.view(N, V, k, C)\n        x = x.unsqueeze(2).expand(-1, -1, k, -1)\n        \n        feature = torch.cat((feature - x, x), dim=3)\n        feature = feature.permute(0, 3, 1, 2).contiguous()\n        \n        return feature\n    \n\nclass AHA(nn.Module):\n    def __init__(self, in_channels, num_layers, CoM):\n        super(AHA, self).__init__()\n        \n        self.num_layers = num_layers\n        \n        groups = get_groups(dataset='NTU', CoM=CoM)\n\n        for i, group in enumerate(groups):\n            group = [i - 1 for i in group]\n            groups[i] = group\n            \n        inter_channels = in_channels // 4\n            \n        self.layers = [groups[i] + groups[i + 1] for i in range(len(groups) - 1)]\n        \n        self.conv_down = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, kernel_size=1),\n            nn.BatchNorm2d(inter_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.edge_conv = EdgeConv(inter_channels, inter_channels, k=3)\n        \n        self.aggregate = nn.Conv1d(inter_channels, in_channels, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n        \n        \n        \n    def forward(self, x):\n        N, C, L, T, V = x.size()\n        \n        x_t = x.max(dim=-2, keepdim=False)[0]\n        x_t = self.conv_down(x_t)\n        \n        x_sampled = []\n        for i in range(self.num_layers):\n            s_t = x_t[:, :, i, self.layers[i]]\n            s_t = s_t.mean(dim=-1, keepdim=True)\n            x_sampled.append(s_t)\n        x_sampled = torch.cat(x_sampled, dim=2)\n        \n        att = self.edge_conv(x_sampled, dim=3)\n        att = self.aggregate(att).view(N, C, L, 1, 1)\n        \n        out = (x * self.sigmoid(att)).sum(dim=2, keepdim=False)\n        \n        return out\n        \n\n\nclass HD_Gconv(nn.Module):\n    def __init__(self, in_channels, out_channels, A, adaptive=True, residual=True, att=False, CoM=8):\n        super(HD_Gconv, self).__init__()\n        self.num_layers = A.shape[0]\n        self.num_subset = A.shape[1]\n        \n        self.att = att\n        \n        inter_channels = out_channels // (self.num_subset + 1)\n        self.adaptive = adaptive\n        \n        if adaptive:\n            self.PA = nn.Parameter(torch.from_numpy(A.astype(np.float32)), requires_grad=True)\n        else:\n            raise ValueError()\n\n        self.conv_down = nn.ModuleList()\n        self.conv = nn.ModuleList()\n        for i in range(self.num_layers):\n            self.conv_d = nn.ModuleList()\n            self.conv_down.append(nn.Sequential(\n                nn.Conv2d(in_channels, inter_channels, kernel_size=1),\n                nn.BatchNorm2d(inter_channels),\n                nn.ReLU(inplace=True)\n            ))\n            for j in range(self.num_subset):\n                self.conv_d.append(nn.Sequential(\n                    nn.Conv2d(inter_channels, inter_channels, kernel_size=1),\n                    nn.BatchNorm2d(inter_channels)\n                ))\n\n            self.conv_d.append(EdgeConv(inter_channels, inter_channels, k=5))\n            self.conv.append(self.conv_d)\n            \n        if self.att:\n            self.aha = AHA(out_channels, num_layers=self.num_layers, CoM=CoM)\n            \n        if residual:\n            if in_channels != out_channels:\n                self.down = nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels, 1),\n                    nn.BatchNorm2d(out_channels)\n                )\n            else:\n                self.down = lambda x: x\n        else:\n            self.down = lambda x: 0\n            \n        self.bn = nn.BatchNorm2d(out_channels)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                conv_init(m)\n            elif isinstance(m, nn.BatchNorm2d):\n                bn_init(m, 1)\n        bn_init(self.bn, 1e-6)\n\n    def forward(self, x):\n        A = self.PA\n\n        out = []\n        for i in range(self.num_layers):\n            y = []\n            x_down = self.conv_down[i](x)\n            for j in range(self.num_subset):\n                z = torch.einsum('n c t u, v u -> n c t v', x_down, A[i, j])\n                z = self.conv[i][j](z)\n                y.append(z)\n            y_edge = self.conv[i][-1](x_down)\n            y.append(y_edge)\n            y = torch.cat(y, dim=1)\n            \n            out.append(y)\n            \n        out = torch.stack(out, dim=2)\n        if self.att:\n            out = self.aha(out)\n        else:\n            out = out.sum(dim=2, keepdim=False)\n            \n        out = self.bn(out)\n        \n        out += self.down(x)\n        out = self.relu(out)\n\n        return out\n\nclass TCN_GCN_unit(nn.Module):\n    def __init__(self, in_channels, out_channels, A, stride=1, residual=True, adaptive=True,\n                 kernel_size=5, dilations=[1, 2], att=True, CoM=8):\n        super(TCN_GCN_unit, self).__init__()\n        self.gcn1 = HD_Gconv(in_channels, out_channels, A, adaptive=adaptive, att=att, CoM=CoM)\n        self.tcn1 = MultiScale_TemporalConv(out_channels, out_channels, kernel_size=kernel_size, stride=stride, dilations=dilations,\n                                            residual=False)\n        self.relu = nn.ReLU(inplace=True)\n        if not residual:\n            self.residual = lambda x: 0\n\n        elif (in_channels == out_channels) and (stride == 1):\n            self.residual = lambda x: x\n\n        else:\n            self.residual = residual_conv(in_channels, out_channels, kernel_size=1, stride=stride)\n\n    def forward(self, x):\n        y = self.relu(self.tcn1(self.gcn1(x)) + self.residual(x))\n        return y\n\n\nclass Model(nn.Module):\n    def __init__(self, num_class=3, num_point=17, num_person=1, graph=Graph, graph_args={'labeling_mode': 'spatial','CoM': 8}, in_channels=3,\n                 drop_out=0, adaptive=True):\n        super(Model, self).__init__()\n\n        if graph_args is None:\n            graph_args = dict()\n        if graph is None:\n            raise ValueError()\n        else:\n            self.graph = Graph(**graph_args)\n            A, CoM = self.graph.A\n        \n        self.dataset = 'NTU' #if num_point == 25 else 'FineFS' if num_point == 17 else 'UCLA'\n\n        self.num_class = num_class\n        self.num_point = num_point\n        self.data_bn = nn.BatchNorm1d(num_person * in_channels * num_point)\n\n        base_channels = 64\n\n        self.l1 = TCN_GCN_unit(3, base_channels, A, residual=False, adaptive=adaptive, att=False, CoM=CoM)\n        self.l2 = TCN_GCN_unit(base_channels, base_channels, A, adaptive=adaptive, CoM=CoM)\n        self.l3 = TCN_GCN_unit(base_channels, base_channels, A, adaptive=adaptive, CoM=CoM)\n        self.l4 = TCN_GCN_unit(base_channels, base_channels, A, adaptive=adaptive, CoM=CoM)\n        self.l5 = TCN_GCN_unit(base_channels, base_channels*2, A, stride=2, adaptive=adaptive, CoM=CoM)\n        self.l6 = TCN_GCN_unit(base_channels*2, base_channels*2, A, adaptive=adaptive, CoM=CoM)\n        self.l7 = TCN_GCN_unit(base_channels*2, base_channels*2, A, adaptive=adaptive, CoM=CoM)\n        self.l8 = TCN_GCN_unit(base_channels*2, base_channels*4, A, stride=2, adaptive=adaptive, CoM=CoM)\n        self.l9 = TCN_GCN_unit(base_channels*4, base_channels*4, A, adaptive=adaptive, CoM=CoM)\n        self.l10 = TCN_GCN_unit(base_channels*4, base_channels*4, A, adaptive=adaptive, CoM=CoM)\n\n        self.fc = nn.Linear(base_channels*4, num_class)\n\n        nn.init.normal_(self.fc.weight, 0, math.sqrt(2. / num_class))\n        bn_init(self.data_bn, 1)\n        if drop_out:\n            self.drop_out = nn.Dropout(drop_out)\n        else:\n            self.drop_out = lambda x: x\n\n    def forward(self, x):\n        N, C, T, V, M = x.size()\n        x = x.permute(0, 4, 3, 1, 2).contiguous().view(N, M * V * C, T)\n        x = self.data_bn(x)\n        x = x.view(N, M, V, C, T).permute(0, 1, 3, 4, 2).contiguous().view(N * M, C, T, V)\n\n        x = self.l1(x)\n        x = self.l2(x)\n        x = self.l3(x)\n        x = self.l4(x)\n        x = self.l5(x)\n        x = self.l6(x)\n        x = self.l7(x)\n        x = self.l8(x)\n        x = self.l9(x)\n        x = self.l10(x)\n\n        # N*M,C,T,V\n        c_new = x.size(1)\n        x = x.view(N, M, c_new, -1)\n        x = x.mean(3).mean(1)\n        x = self.drop_out(x)\n\n        return self.fc(x)","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_212","output_type":"execute_reply","data":{"status":"ok","execution_count":5,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:17:06.113211Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_212","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.136890Z","version":"5.3"}}],"execution_count":5},{"id":"dc82a923-d941-47a6-a6c6-98d62b8f1ce6","cell_type":"code","source":"import torch.nn.functional as F\nclass SimpleSkeletonDataset(Dataset):\n    def __init__(self, x, y):\n        # 转换数据格式: (n, c, t, v, m) -> (n, c, t, v)\n        self.x = x[:, :, :64, :, :]\n        # 转换标签: (n, 3) -> (n,) 类别索引\n        self.y = np.argmax(y, axis=1)  # 从one-hot转类别索引\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        return torch.FloatTensor(self.x[idx]), torch.LongTensor([self.y[idx]]).squeeze()\n        \nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        confidence = 1. - self.smoothing\n        logprobs = F.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_216","output_type":"execute_reply","data":{"status":"ok","execution_count":6,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:17:06.137744Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_216","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.141215Z","version":"5.3"}}],"execution_count":6},{"id":"59534977-fdfc-4bb8-82a7-c06e168657dd","cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\n\ndef valid_crop_resize(data_numpy,valid_frame_num,p_interval,window):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    begin = 0\n    end = valid_frame_num\n    valid_size = end - begin\n\n    #crop\n    if len(p_interval) == 1:\n        p = p_interval[0]\n        bias = int((1-p) * valid_size/2)\n        data = data_numpy[:, begin+bias:end-bias, :, :]# center_crop\n        cropped_length = data.shape[1]\n    else:\n        p = np.random.rand(1)*(p_interval[1]-p_interval[0])+p_interval[0]\n        cropped_length = np.minimum(np.maximum(int(np.floor(valid_size*p)),64), valid_size)# constraint cropped_length lower bound as 64\n        bias = np.random.randint(0,valid_size-cropped_length+1)\n        data = data_numpy[:, begin+bias:begin+bias+cropped_length, :, :]\n        if data.shape[1] == 0:\n            print(cropped_length, bias, valid_size)\n\n    # resize\n    data = torch.tensor(data,dtype=torch.float)\n    data = data.permute(0, 2, 3, 1).contiguous().view(C * V * M, cropped_length)\n    data = data[None, None, :, :]\n    data = F.interpolate(data, size=(C * V * M, window), mode='bilinear',align_corners=False).squeeze() # could perform both up sample and down sample\n    data = data.contiguous().view(C, V, M, window).permute(0, 3, 1, 2).contiguous().numpy()\n\n    return data\n\ndef downsample(data_numpy, step, random_sample=True):\n    # input: C,T,V,M\n    begin = np.random.randint(step) if random_sample else 0\n    return data_numpy[:, begin::step, :, :]\n\n\ndef temporal_slice(data_numpy, step):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    return data_numpy.reshape(C, T / step, step, V, M).transpose(\n        (0, 1, 3, 2, 4)).reshape(C, T / step, V, step * M)\n\n\ndef mean_subtractor(data_numpy, mean):\n    # input: C,T,V,M\n    # naive version\n    if mean == 0:\n        return\n    C, T, V, M = data_numpy.shape\n    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0\n    begin = valid_frame.argmax()\n    end = len(valid_frame) - valid_frame[::-1].argmax()\n    data_numpy[:, :end, :, :] = data_numpy[:, :end, :, :] - mean\n    return data_numpy\n\n\ndef auto_pading(data_numpy, size, random_pad=False):\n    C, T, V, M = data_numpy.shape\n    if T < size:\n        begin = random.randint(0, size - T) if random_pad else 0\n        data_numpy_paded = np.zeros((C, size, V, M))\n        data_numpy_paded[:, begin:begin + T, :, :] = data_numpy\n        return data_numpy_paded\n    else:\n        return data_numpy\n\n\ndef random_choose(data_numpy, size, auto_pad=True):\n    # input: C,T,V,M 随机选择其中一段，不是很合理。因为有0\n    C, T, V, M = data_numpy.shape\n    if T == size:\n        return data_numpy\n    elif T < size:\n        if auto_pad:\n            return auto_pading(data_numpy, size, random_pad=True)\n        else:\n            return data_numpy\n    else:\n        begin = random.randint(0, T - size)\n        return data_numpy[:, begin:begin + size, :, :]\n\ndef random_move(data_numpy,\n                angle_candidate=[-10., -5., 0., 5., 10.],\n                scale_candidate=[0.9, 1.0, 1.1],\n                transform_candidate=[-0.2, -0.1, 0.0, 0.1, 0.2],\n                move_time_candidate=[1]):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    move_time = random.choice(move_time_candidate)\n    node = np.arange(0, T, T * 1.0 / move_time).round().astype(int)\n    node = np.append(node, T)\n    num_node = len(node)\n\n    A = np.random.choice(angle_candidate, num_node)\n    S = np.random.choice(scale_candidate, num_node)\n    T_x = np.random.choice(transform_candidate, num_node)\n    T_y = np.random.choice(transform_candidate, num_node)\n\n    a = np.zeros(T)\n    s = np.zeros(T)\n    t_x = np.zeros(T)\n    t_y = np.zeros(T)\n\n    # linspace\n    for i in range(num_node - 1):\n        a[node[i]:node[i + 1]] = np.linspace(\n            A[i], A[i + 1], node[i + 1] - node[i]) * np.pi / 180\n        s[node[i]:node[i + 1]] = np.linspace(S[i], S[i + 1],\n                                             node[i + 1] - node[i])\n        t_x[node[i]:node[i + 1]] = np.linspace(T_x[i], T_x[i + 1],\n                                               node[i + 1] - node[i])\n        t_y[node[i]:node[i + 1]] = np.linspace(T_y[i], T_y[i + 1],\n                                               node[i + 1] - node[i])\n\n    theta = np.array([[np.cos(a) * s, -np.sin(a) * s],\n                      [np.sin(a) * s, np.cos(a) * s]])\n\n    # perform transformation\n    for i_frame in range(T):\n        xy = data_numpy[0:2, i_frame, :, :]\n        new_xy = np.dot(theta[:, :, i_frame], xy.reshape(2, -1))\n        new_xy[0] += t_x[i_frame]\n        new_xy[1] += t_y[i_frame]\n        data_numpy[0:2, i_frame, :, :] = new_xy.reshape(2, V, M)\n\n    return data_numpy\n\n\ndef random_shift(data_numpy):\n    C, T, V, M = data_numpy.shape\n    data_shift = np.zeros(data_numpy.shape)\n    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0\n    begin = valid_frame.argmax()\n    end = len(valid_frame) - valid_frame[::-1].argmax()\n\n    size = end - begin\n    bias = random.randint(0, T - size)\n    data_shift[:, bias:bias + size, :, :] = data_numpy[:, begin:end, :, :]\n\n    return data_shift\n\n\ndef _rot(rot):\n    \"\"\"\n    rot: T,3\n    \"\"\"\n    cos_r, sin_r = rot.cos(), rot.sin()  # T,3\n    zeros = torch.zeros(rot.shape[0], 1)  # T,1\n    ones = torch.ones(rot.shape[0], 1)  # T,1\n\n    r1 = torch.stack((ones, zeros, zeros),dim=-1)  # T,1,3\n    rx2 = torch.stack((zeros, cos_r[:,0:1], sin_r[:,0:1]), dim = -1)  # T,1,3\n    rx3 = torch.stack((zeros, -sin_r[:,0:1], cos_r[:,0:1]), dim = -1)  # T,1,3\n    rx = torch.cat((r1, rx2, rx3), dim = 1)  # T,3,3\n\n    ry1 = torch.stack((cos_r[:,1:2], zeros, -sin_r[:,1:2]), dim =-1)\n    r2 = torch.stack((zeros, ones, zeros),dim=-1)\n    ry3 = torch.stack((sin_r[:,1:2], zeros, cos_r[:,1:2]), dim =-1)\n    ry = torch.cat((ry1, r2, ry3), dim = 1)\n\n    rz1 = torch.stack((cos_r[:,2:3], sin_r[:,2:3], zeros), dim =-1)\n    r3 = torch.stack((zeros, zeros, ones),dim=-1)\n    rz2 = torch.stack((-sin_r[:,2:3], cos_r[:,2:3],zeros), dim =-1)\n    rz = torch.cat((rz1, rz2, r3), dim = 1)\n\n    rot = rz.matmul(ry).matmul(rx)\n    return rot\n\n\ndef random_rot(data_numpy, theta=0.3):\n    \"\"\"\n    data_numpy: C,T,V,M\n    \"\"\"\n    data_torch = torch.from_numpy(data_numpy)\n    C, T, V, M = data_torch.shape\n    data_torch = data_torch.permute(1, 0, 2, 3).contiguous().view(T, C, V*M)  # T,3,V*M\n    rot = torch.zeros(3).uniform_(-theta, theta)\n    rot = torch.stack([rot, ] * T, dim=0)\n    rot = _rot(rot)  # T,3,3\n    data_torch = torch.matmul(rot, data_torch)\n    data_torch = data_torch.view(T, C, V, M).permute(1, 0, 2, 3).contiguous()\n\n    return data_torch\n\ndef openpose_match(data_numpy):\n    C, T, V, M = data_numpy.shape\n    assert (C == 3)\n    score = data_numpy[2, :, :, :].sum(axis=1)\n    # the rank of body confidence in each frame (shape: T-1, M)\n    rank = (-score[0:T - 1]).argsort(axis=1).reshape(T - 1, M)\n\n    # data of frame 1\n    xy1 = data_numpy[0:2, 0:T - 1, :, :].reshape(2, T - 1, V, M, 1)\n    # data of frame 2\n    xy2 = data_numpy[0:2, 1:T, :, :].reshape(2, T - 1, V, 1, M)\n    # square of distance between frame 1&2 (shape: T-1, M, M)\n    distance = ((xy2 - xy1) ** 2).sum(axis=2).sum(axis=0)\n\n    # match pose\n    forward_map = np.zeros((T, M), dtype=int) - 1\n    forward_map[0] = range(M)\n    for m in range(M):\n        choose = (rank == m)\n        forward = distance[choose].argmin(axis=1)\n        for t in range(T - 1):\n            distance[t, :, forward[t]] = np.inf\n        forward_map[1:][choose] = forward\n    assert (np.all(forward_map >= 0))\n\n    # string data\n    for t in range(T - 1):\n        forward_map[t + 1] = forward_map[t + 1][forward_map[t]]\n\n    # generate data\n    new_data_numpy = np.zeros(data_numpy.shape)\n    for t in range(T):\n        new_data_numpy[:, t, :, :] = data_numpy[:, t, :, forward_map[\n                                                             t]].transpose(1, 2, 0)\n    data_numpy = new_data_numpy\n\n    # score sort\n    trace_score = data_numpy[2, :, :, :].sum(axis=1).sum(axis=0)\n    rank = (-trace_score).argsort()\n    data_numpy = data_numpy[:, :, :, rank]\n\n    return data_numpy\n","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_220","output_type":"execute_reply","data":{"status":"ok","execution_count":7,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:17:06.142189Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_220","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.403393Z","version":"5.3"}}],"execution_count":7},{"id":"bc1a98c6-bca6-458e-acaa-5d7ed857b31c","cell_type":"code","source":"class Feeder(Dataset):\n    def __init__(self, data_path, label_path=None, p_interval=1, split='train', random_choose=False, random_shift=False,\n                 random_move=False, random_rot=False, window_size=-1, normalization=False, debug=False, use_mmap=False,\n                 bone=False):\n        \"\"\"\n        :param data_path:\n        :param label_path:\n        :param split: training set or test set\n        :param random_choose: If true, randomly choose a portion of the input sequence\n        :param random_shift: If true, randomly pad zeros at the begining or end of sequence\n        :param random_move:\n        :param random_rot: rotate skeleton around xyz axis\n        :param window_size: The length of the output sequence\n        :param normalization: If true, normalize input sequence\n        :param debug: If true, only use the first 100 samples\n        :param use_mmap: If true, use mmap mode to load data, which can save the running memory\n        :param bone: use bone modality or not\n        :param vel: use motion modality or not\n        :param only_label: only load label for ensemble score compute\n        \"\"\"\n\n        self.debug = debug\n        self.data_path = data_path\n        self.label_path = label_path\n        self.split = split\n        self.random_choose = random_choose\n        self.random_shift = random_shift\n        self.random_move = random_move\n        self.window_size = window_size\n        self.normalization = normalization\n        self.use_mmap = use_mmap\n        self.p_interval = p_interval\n        self.random_rot = random_rot\n        self.bone = bone\n        self.load_data()\n        if normalization:\n            self.get_mean_map()\n\n    def load_data(self):\n        # data: N C V T M\n        npz_data = np.load(self.data_path)\n        if self.split == 'train':\n            self.data = npz_data['x_train']\n            self.label = np.where(npz_data['y_train'] > 0)[1]\n            self.sample_name = ['train_' + str(i) for i in range(len(self.data))]\n        elif self.split == 'test':\n            self.data = npz_data['x_test']\n            self.label = np.where(npz_data['y_test'] > 0)[1]\n            self.sample_name = ['test_' + str(i) for i in range(len(self.data))]\n        else:\n            raise NotImplementedError('data split only supports train/test')\n\n    def get_mean_map(self):\n        data = self.data\n        N, C, T, V, M = data.shape\n        self.mean_map = data.mean(axis=2, keepdims=True).mean(axis=4, keepdims=True).mean(axis=0)\n        self.std_map = data.transpose((0, 2, 4, 1, 3)).reshape((N * T * M, C * V)).std(axis=0).reshape((C, 1, V, 1))\n\n    def __len__(self):\n        return len(self.label)\n\n    def __iter__(self):\n        return self\n\n    def __getitem__(self, index):\n        data_numpy = self.data[index]\n        label = self.label[index]\n        data_numpy = np.array(data_numpy)\n        valid_frame_num = np.sum(data_numpy.sum(0).sum(-1).sum(-1) != 0)\n        if self.random_rot:\n            data_numpy = random_rot(data_numpy)\n        if self.bone:\n            from .bone_pairs import ntu_pairs\n            bone_data_numpy = np.zeros_like(data_numpy) # 3, T, V\n            for v1, v2 in ntu_pairs:\n                bone_data_numpy[:, :, v1 - 1] = data_numpy[:, :, v1 - 1] - data_numpy[:, :, v2 - 1]\n            data_numpy = bone_data_numpy\n        return data_numpy[:, :64, :, :], label, index\n\n    def top_k(self, score, top_k):\n        rank = score.argsort()\n        hit_top_k = [l in rank[i, -top_k:] for i, l in enumerate(self.label)]\n        return sum(hit_top_k) * 1.0 / len(hit_top_k)\n\n\ndef import_class(name):\n    components = name.split('.')\n    mod = __import__(components[0])\n    for comp in components[1:]:\n        mod = getattr(mod, comp)\n    return mod","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_224","output_type":"execute_reply","data":{"status":"ok","execution_count":8,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:17:06.404543Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_224","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.411475Z","version":"5.3"}}],"execution_count":8},{"id":"5623ab82-c995-4fec-bdad-3bd1edeff3ca","cell_type":"code","source":"","metadata":{},"execution_count":null},{"id":"35abe85b-39f2-4f5a-96b9-caaaa2830721","cell_type":"code","source":"import math\n'''\nif __name__ == '__main__':\n    # 初始化\n    init_seed(1)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # 加载数据\n    print(\"Loading data...\")\n    data = np.load('/bohr/finefs-9pcr/v1/FS_data_for_HDGCN_3_cat.npz')\n    train_set = SimpleSkeletonDataset(data['x_train'], data['y_train'])\n    test_set = SimpleSkeletonDataset(data['x_test'], data['y_test'])\n    \n    # 数据加载器\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4)\n    test_loader = DataLoader(test_set, batch_size=64, num_workers=4)\n    \n    # 模型和优化器\n    print(\"Initializing model...\")\n    model = Model().to(device)\n    criterion = LabelSmoothingCrossEntropy().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay = 0.0004)\n    \n    # 训练循环\n    print(\"Starting training...\")\n    for epoch in range(20):  # 简化为10个epoch\n        train_loss, train_acc = simple_train(model, train_loader, criterion, optimizer, device)\n        test_loss, test_acc = simple_test(model, test_loader, criterion, device)\n        \n        print(f'\\nEpoch {epoch+1}:')\n        print(f'Train Loss: {train_loss:.4f}, Acc: {train_acc:.2%}')\n        print(f'Test Loss: {test_loss:.4f}, Acc: {test_acc:.2%}')'''","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_228","output_type":"execute_result","data":{"text/plain":"'\\nif __name__ == \\'__main__\\':\\n    # 初始化\\n    init_seed(1)\\n    device = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n    print(f\"Using device: {device}\")\\n    \\n    # 加载数据\\n    print(\"Loading data...\")\\n    data = np.load(\\'/bohr/finefs-9pcr/v1/FS_data_for_HDGCN_3_cat.npz\\')\\n    train_set = SimpleSkeletonDataset(data[\\'x_train\\'], data[\\'y_train\\'])\\n    test_set = SimpleSkeletonDataset(data[\\'x_test\\'], data[\\'y_test\\'])\\n    \\n    # 数据加载器\\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4)\\n    test_loader = DataLoader(test_set, batch_size=64, num_workers=4)\\n    \\n    # 模型和优化器\\n    print(\"Initializing model...\")\\n    model = Model().to(device)\\n    criterion = LabelSmoothingCrossEntropy().to(device)\\n    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay = 0.0004)\\n    \\n    # 训练循环\\n    print(\"Starting training...\")\\n    for epoch in range(20):  # 简化为10个epoch\\n        train_loss, train_acc = simple_train(model, train_loader, criterion, optimizer, device)\\n        test_loss, test_acc = simple_test(model, test_loader, criterion, device)\\n        \\n        print(f\\'\\nEpoch {epoch+1}:\\')\\n        print(f\\'Train Loss: {train_loss:.4f}, Acc: {train_acc:.2%}\\')\\n        print(f\\'Test Loss: {test_loss:.4f}, Acc: {test_acc:.2%}\\')'"},"meta":{},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_228","msg_type":"execute_result","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.413887Z","version":"5.3"}},{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_229","output_type":"execute_reply","data":{"status":"ok","execution_count":9,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:17:06.412482Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_229","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.416872Z","version":"5.3"}}],"execution_count":9},{"id":"71fbad96-5c67-49f2-9dc0-852a8413221a","cell_type":"code","source":"class Processor():\n    \"\"\" \n        Processor for Skeleton-based Action Recgnition\n    \"\"\"\n\n    def __init__(self):\n        self.train_feeder_args = {\n            'data_path': '/bohr/finefs-9pcr/v3/FS_data_for_HDGCN_jump.npz',\n            'split': 'train',\n            'debug': False,\n            'random_choose': False,\n            'random_shift': False,\n            'random_move': False,\n            'window_size': 64,\n            'normalization': False,\n            'random_rot': True,\n            'p_interval': [0.5, 1],\n            'bone': False\n        }\n        \n        self.test_feeder_args = {\n            'data_path': '/bohr/finefs-9pcr/v3/FS_data_for_HDGCN_jump.npz',\n            'split': 'test',\n            'window_size': 64,\n            'p_interval': [0.95],\n            'bone': False,\n            'debug': False\n        }\n        self.load_model()\n        self.load_optimizer()\n        self.load_data()\n        self.lr = 0.01\n        self.best_acc = 0\n        self.best_acc_epoch = 0\n        self.num_epoch = 20\n        self.model = self.model.cuda(0)\n        \n\n    def load_data(self):\n        self.data_loader = dict()\n        self.data_loader['train'] = torch.utils.data.DataLoader(\n            dataset=Feeder(**self.train_feeder_args),\n            batch_size=64,\n            shuffle=True,\n            num_workers=4,\n            drop_last=True,\n            worker_init_fn=init_seed)\n        self.data_loader['test'] = torch.utils.data.DataLoader(\n            dataset=Feeder(**self.test_feeder_args),\n            batch_size=8,\n            shuffle=False,\n            num_workers=4,\n            drop_last=False,\n            worker_init_fn=init_seed)\n\n    def load_model(self):\n        self.model = Model(17)\n        self.loss = LabelSmoothingCrossEntropy(smoothing=0.1).cuda(0)\n\n    def load_optimizer(self):\n        self.optimizer = optim.Adam(\n            self.model.parameters(),\n            lr=0.01,\n            weight_decay=0.0004)\n\n    def adjust_learning_rate(self, epoch, idx):\n        T_max = len(self.data_loader['train']) * (self.num_epoch)\n        T_cur = len(self.data_loader['train']) * (epoch) + idx\n\n        eta_min = 0.01 * 0.001\n        lr = eta_min + 0.5 * (0.01 - eta_min) * (1 + np.cos((T_cur / T_max) * np.pi))\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        return lr\n\n    \n    def train(self, epoch, save_model=False):\n        self.model.train()\n        loader = self.data_loader['train']\n\n        loss_value = []\n        acc_value = []\n        process = tqdm(loader)\n        for batch_idx, (data, label, index) in enumerate(process):\n\n            self.adjust_learning_rate(epoch, batch_idx)\n            self.global_step += 1\n            with torch.no_grad():\n                data = data.float().cuda(0)\n                label = label.long().cuda(0)\n\n            # forward\n            output = self.model(data)\n            loss = self.loss(output, label)\n            # backward\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            loss_value.append(loss.data.item())\n\n            value, predict_label = torch.max(output.data, 1)\n            acc = torch.mean((predict_label == label.data).float())\n            acc_value.append(acc.data.item())\n            \n            self.lr = self.optimizer.param_groups[0]['lr']\n        print(f'Train Loss: {np.mean(loss_value):.4f}, Acc: {np.mean(acc_value):.2%}')\n    def eval(self, epoch, save_score=False, loader_name=['test']):\n        self.model.eval()\n        print('Eval epoch: {}'.format(epoch + 1))\n        for ln in loader_name:\n            loss_value = []\n            score_frag = []\n            label_list = []\n            pred_list = []\n            step = 0\n            process = tqdm(self.data_loader[ln])\n            for batch_idx, (data, label, index) in enumerate(process):\n                label_list.append(label)\n                with torch.no_grad():\n                    data = data.float().cuda(0)\n                    label = label.long().cuda(0)\n                    output = self.model(data)\n                    loss = self.loss(output, label)\n                    score_frag.append(output.data.cpu().numpy())\n                    loss_value.append(loss.data.item())\n\n                    _, predict_label = torch.max(output.data, 1)\n                    pred_list.append(predict_label.data.cpu().numpy())\n                    step += 1\n            score = np.concatenate(score_frag)\n            loss = np.mean(loss_value)\n            accuracy = self.data_loader[ln].dataset.top_k(score, 1)\n            if accuracy > self.best_acc:\n                self.best_acc = accuracy\n                self.best_acc_epoch = epoch + 1\n\n            print('Accuracy: ', accuracy)\n\n    def start(self):\n        self.global_step = 0\n        for epoch in range(0, self.num_epoch):\n\n            self.train(epoch, save_model=True)\n            self.eval(epoch, save_score=True, loader_name=['test'])","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_233","output_type":"execute_reply","data":{"status":"ok","execution_count":10,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:17:06.418241Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_233","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.426068Z","version":"5.3"}}],"execution_count":10},{"id":"e9102e24-7246-4114-a418-af624cebfcd0","cell_type":"code","source":"processor = Processor()\nprocessor.start()","metadata":{},"outputs":[{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_237","output_type":"stream","name":"stderr","text":"100%|██████████| 48/48 [00:13<00:00,  3.48it/s]\nTrain Loss: 3.2723, Acc: 18.88%\nEval epoch: 1\n100%|██████████| 21/21 [00:01<00:00, 16.38it/s]\nAccuracy:  0.23780487804878048\n100%|██████████| 48/48 [00:13<00:00,  3.61it/s]\nTrain Loss: 2.3984, Acc: 24.54%\nEval epoch: 2\n100%|██████████| 21/21 [00:01<00:00, 17.13it/s]\nAccuracy:  0.23780487804878048\n100%|██████████| 48/48 [00:13<00:00,  3.63it/s]\nTrain Loss: 2.1853, Acc: 34.02%\nEval epoch: 3\n100%|██████████| 21/21 [00:01<00:00, 17.62it/s]\nAccuracy:  0.4268292682926829\n100%|██████████| 48/48 [00:13<00:00,  3.59it/s]\nTrain Loss: 2.0468, Acc: 39.81%\nEval epoch: 4\n100%|██████████| 21/21 [00:01<00:00, 17.64it/s]\nAccuracy:  0.3170731707317073\n100%|██████████| 48/48 [00:13<00:00,  3.60it/s]\nTrain Loss: 1.9079, Acc: 43.88%\nEval epoch: 5\n100%|██████████| 21/21 [00:01<00:00, 17.86it/s]\nAccuracy:  0.4024390243902439\n100%|██████████| 48/48 [00:13<00:00,  3.56it/s]\nTrain Loss: 1.8363, Acc: 47.85%\nEval epoch: 6\n100%|██████████| 21/21 [00:01<00:00, 17.86it/s]\nAccuracy:  0.43902439024390244\n100%|██████████| 48/48 [00:13<00:00,  3.61it/s]\nTrain Loss: 1.7727, Acc: 49.51%\nEval epoch: 7\n100%|██████████| 21/21 [00:01<00:00, 17.61it/s]\nAccuracy:  0.5121951219512195\n100%|██████████| 48/48 [00:13<00:00,  3.59it/s]\nTrain Loss: 1.6957, Acc: 53.55%\nEval epoch: 8\n100%|██████████| 21/21 [00:01<00:00, 17.16it/s]\nAccuracy:  0.5304878048780488\n100%|██████████| 48/48 [00:13<00:00,  3.57it/s]\nTrain Loss: 1.6012, Acc: 58.43%\nEval epoch: 9\n100%|██████████| 21/21 [00:01<00:00, 17.47it/s]\nAccuracy:  0.5487804878048781\n100%|██████████| 48/48 [00:13<00:00,  3.62it/s]\nTrain Loss: 1.5308, Acc: 61.30%\nEval epoch: 10\n100%|██████████| 21/21 [00:01<00:00, 17.61it/s]\nAccuracy:  0.524390243902439\n100%|██████████| 48/48 [00:13<00:00,  3.59it/s]\nTrain Loss: 1.4884, Acc: 63.18%\nEval epoch: 11\n100%|██████████| 21/21 [00:01<00:00, 17.67it/s]\nAccuracy:  0.5182926829268293\n100%|██████████| 48/48 [00:13<00:00,  3.57it/s]\nTrain Loss: 1.4291, Acc: 66.41%\nEval epoch: 12\n100%|██████████| 21/21 [00:01<00:00, 17.76it/s]\nAccuracy:  0.6280487804878049\n100%|██████████| 48/48 [00:13<00:00,  3.61it/s]\nTrain Loss: 1.3630, Acc: 69.08%\nEval epoch: 13\n100%|██████████| 21/21 [00:01<00:00, 16.77it/s]\nAccuracy:  0.6280487804878049\n100%|██████████| 48/48 [00:13<00:00,  3.56it/s]\nTrain Loss: 1.2824, Acc: 72.98%\nEval epoch: 14\n100%|██████████| 21/21 [00:01<00:00, 17.28it/s]\nAccuracy:  0.6585365853658537\n100%|██████████| 48/48 [00:13<00:00,  3.61it/s]\nTrain Loss: 1.2055, Acc: 75.65%\nEval epoch: 15\n100%|██████████| 21/21 [00:01<00:00, 17.15it/s]\nAccuracy:  0.6463414634146342\n100%|██████████| 48/48 [00:13<00:00,  3.62it/s]\nTrain Loss: 1.1367, Acc: 79.85%\nEval epoch: 16\n100%|██████████| 21/21 [00:01<00:00, 17.68it/s]\nAccuracy:  0.6219512195121951\n100%|██████████| 48/48 [00:13<00:00,  3.56it/s]\nTrain Loss: 1.0683, Acc: 82.62%\nEval epoch: 17\n100%|██████████| 21/21 [00:01<00:00, 17.81it/s]\nAccuracy:  0.6524390243902439\n100%|██████████| 48/48 [00:13<00:00,  3.59it/s]\nTrain Loss: 1.0194, Acc: 84.93%\nEval epoch: 18\n100%|██████████| 21/21 [00:01<00:00, 17.76it/s]\nAccuracy:  0.676829268292683\n100%|██████████| 48/48 [00:13<00:00,  3.59it/s]\nTrain Loss: 0.9672, Acc: 87.86%\nEval epoch: 19\n100%|██████████| 21/21 [00:01<00:00, 17.64it/s]\nAccuracy:  0.6524390243902439\n100%|██████████| 48/48 [00:13<00:00,  3.63it/s]\nTrain Loss: 0.9527, Acc: 88.28%\nEval epoch: 20\n100%|██████████| 21/21 [00:01<00:00, 17.46it/s]Accuracy:  0.6402439024390244\n\n","data":{"name":"stderr","text":"\r  0%|          | 0/48 [00:00<?, ?it/s]"},"meta":{},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_237","msg_type":"stream","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:17:06.732978Z","version":"5.3"}},{"id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_1577","output_type":"execute_reply","data":{"status":"ok","execution_count":11,"user_expressions":{},"payload":[]},"meta":{"started":"2025-07-11T08:17:06.426916Z","dependencies_met":true,"engine":"b2970641-d11a-4288-bbad-be6566515074","status":"ok"},"parent_header":{"msg_id":"ccc8a6d9-24bdd8ba7d8d74ba752dd636_1047_1577","msg_type":"execute_reply","username":"username","session":"ccc8a6d9-24bdd8ba7d8d74ba752dd636","date":"2025-07-11T08:21:58.539137Z","version":"5.3"}}],"execution_count":11},{"id":"1755f838-06d6-407c-9cfc-d62a5920567a","cell_type":"code","source":"","metadata":{},"execution_count":null}],"metadata":{},"nbformat":4,"nbformat_minor":5}