{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d6a6f-7105-45f0-9468-203ea01d7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eff6a0-5287-41cf-9f59-7372a559737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_data(file_path, max_samples=None):\n",
    "    \"\"\"加载并过滤数据，只保留realCount=1的数据\"\"\"\n",
    "    filtered_data = []\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"文件 {file_path} 不存在\")\n",
    "        return []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                if data['realCount'] == 1:\n",
    "                    filtered_data.append(data)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    print(f\"从 {file_path} 中过滤出 {len(filtered_data)} 条 realCount=1 的数据\")\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f2437-2eb5-466a-8176-866e427cb96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_and_filter_data('/personal/Day4/idiom_cloze_project/dataset_filtered/train_data.txt', max_samples=1000)\n",
    "test_data = load_and_filter_data('/personal/Day4/idiom_cloze_project/dataset_filtered/test_data.txt', max_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7d029-dc8a-4d6c-936f-6c733df336f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPODataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data_list = []\n",
    "\n",
    "        for idx in range (len(data)):\n",
    "            content = data[idx][\"content\"].replace(\"#idiom#\",\"[MASK][MASK][MASK][MASK]\")\n",
    "            content = self.tokenizer(content,max_length=self.max_length,padding=\"max_length\",return_tensors=\"pt\")\n",
    "            groundTruth = data[idx][\"groundTruth\"][0]\n",
    "            groundTruth = self.tokenizer.encode(groundTruth)\n",
    "            candidate_encodings = []\n",
    "            for cand_idx in range (7):\n",
    "                if data[idx][\"candidates\"][0][cand_idx] == data[idx][\"groundTruth\"][0]:\n",
    "                    continue\n",
    "                candidate = data[idx][\"candidates\"][0][cand_idx]\n",
    "                candidate = self.tokenizer.encode(candidate)\n",
    "                candidate = torch.LongTensor(candidate)\n",
    "                candidate_encodings.append(candidate)\n",
    "            sample = {\n",
    "                \"groundTruth\": torch.LongTensor(groundTruth)[1:5],\n",
    "                \"candidates\": torch.stack(candidate_encodings)[:,1:5],\n",
    "                \"content_ids\": content[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": content[\"attention_mask\"].squeeze()\n",
    "            }\n",
    "            self.data_list.append(sample)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203fcfd-2ea3-4ccf-9e77-f75abec4a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "model_name = \"/personal/RoBERTa-wwm-ext/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11157754-1055-479b-abf2-87d1488133bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DPODataset(train_data,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f3d34-31f6-49f9-9c95-f43d851416eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=2,num_workers=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2637462-7486-4cd9-b473-add64db20a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DPODataset(train_data,tokenizer)\n",
    "val_loader = torch.utils.data.DataLoader(dataset,batch_size=2,num_workers=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b716b20-1399-422c-9099-ba070a1bcef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, val_loader, epochs = 1):\n",
    "    device = \"cuda\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "    for _ in range (epochs):\n",
    "        total_loss = 0\n",
    "        batch_num = 0 \n",
    "        for batch in dataloader:\n",
    "            model.train()\n",
    "            groundTruth = batch[\"groundTruth\"].to(device)\n",
    "            candidates = batch[\"candidates\"].to(device)\n",
    "            content = batch[\"content_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            output = model(input_ids = content, attention_mask = attention_mask)\n",
    "            mask_token_index = [torch.where(content[i] == tokenizer.mask_token_id) for i in range(content.size(0))]\n",
    "            logits = output.logits\n",
    "            probs = torch.softmax(logits,dim=-1)\n",
    "    \n",
    "            cand_probs = 0\n",
    "            for b in range (content.size(0)):\n",
    "                total_probs = 0\n",
    "                idiom_probs = probs[b][mask_token_index[b][0].tolist()]\n",
    "                for  c in range (candidates.size(1)):\n",
    "                    cand_prob = torch.sum(torch.log(idiom_probs[torch.arange(4),candidates[b][c].tolist()]))\n",
    "                    total_probs += cand_prob\n",
    "                cand_probs += total_probs / candidates.size(1)\n",
    "            cand_probs /= content.size(0)\n",
    "    \n",
    "            gt_probs = 0\n",
    "            for b in range (content.size(0)):\n",
    "                idiom_probs = probs[b][mask_token_index[b][0].tolist()]\n",
    "                gt_probs +=  torch.sum(torch.log(idiom_probs[torch.arange(4),candidates[b][0].tolist()]))\n",
    "            gt_probs /= content.size(0)\n",
    "    \n",
    "            loss = -F.logsigmoid( gt_probs - cand_probs ) * 2.0\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            batch_num += 1\n",
    "            \n",
    "        print(total_loss/batch_num)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        for batch in val_loader:\n",
    "            groundTruth = batch[\"groundTruth\"].to(device)\n",
    "            candidates = batch[\"candidates\"].to(device)\n",
    "            content = batch[\"content_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            output = model(input_ids = content, attention_mask = attention_mask)\n",
    "            mask_token_index = [torch.where(content[i] == tokenizer.mask_token_id) for i in range(content.size(0))]\n",
    "            logits = output.logits\n",
    "            probs = torch.softmax(logits,dim=-1)\n",
    "    \n",
    "            cand_probs = {}\n",
    "            for b in range (content.size(0)):\n",
    "                idiom_probs = probs[b][mask_token_index[b][0].tolist()]\n",
    "                batch_cand_probs = []\n",
    "                for c in range (candidates.size(1)):\n",
    "                    cand_prob = torch.prod(idiom_probs[torch.arange(4),candidates[b][c].tolist()])\n",
    "                    batch_cand_probs.append(cand_prob.detach().cpu().item())\n",
    "                cand_probs[b] = batch_cand_probs\n",
    "    \n",
    "            for b in range (content.size(0)):\n",
    "                idiom_probs = probs[b][mask_token_index[b][0].tolist()]\n",
    "                if torch.prod(idiom_probs[torch.arange(4),candidates[b][0].tolist()]) >= max(cand_probs[b]):\n",
    "                    total_correct += 1\n",
    "    \n",
    "        print(total_correct / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f608f7-9ea6-4621-bcc7-d5e35ca040f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataloader, val_loader, epochs = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
